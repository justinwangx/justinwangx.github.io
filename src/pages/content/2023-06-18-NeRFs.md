---
layout: ../../layouts/Layout.astro
title: "Neural Radiance Fields and Volume Rendering"
description: "An introduction to neural radiance fields"
pubDate: 2023-06-018
---

# Neural Radiance Fields and Volume Rendering

This post aims to be an in-depth explanation of how neural radiance fields (NeRFs) work. The original NeRF paper was released in 2020 by researchers at UC Berkeley, Google Research, and UC San Diego. Since then, there has been lots of follow up work improving on and involving NeRFs, and we are seeing NeRFs becoming more integrated into commercial applications. I think there is room for educational content that sits somewhere between simple introductory blog post and minimal reference implementation--like a written deep dive that introduces NeRFs while still covering, at a granular level, important details such as volume rendering and hierarchical sampling; so here we are.

This is relatively self-contained. The prerequisites I assume are knowledge of ML basics (e.g. multi-layer perceptrons, residual connections, optimization via gradient descent), as well as a conceptual understanding of integration for the section on volume rendering. No knowledge of computer graphics is assumed.

### Overview

NeRFs tackle the problem of novel view synthesis. Given pictures of a scene that are taken at different spatial locations and from varying viewing angles, we can train a NeRF to generate images of the scene from new locations and viewing angles that were not originally captured.

The input of a NeRF is a 5D coordinate consisting of spatial position $\textbf{x} = (x,y,z)$ and viewing direction $\textbf{d} = (\theta, \phi)$. The output of a NeRF is the emitted radiance $\mathbf{c} =(r,g,b)$ and volume density $\sigma$ of the point $\textbf{x}$ when viewed at the angle $\textbf{d}$. We can express this as as $F_{\Theta} : (\textbf{x}, \textbf{d}) \rightarrow (\textbf{c}, \sigma)$, where $\Theta$ are parameters learned via gradient descent.

The NeRF architecture is a simple MLP with one residual connection. After the last full-size linear layer,

Positional encoding.

![NeRF Architecture](/images/nerfs/nerf-architecture.png)

When training a NeRF, we require camera poses that contain the position and viewing direction information for corresponding input images. Sometimes these are known in advance, but often we need to use some sort of structure-from-motion pipeline to acquire them. COLMAP is one such pipeline that is commonly used—we can give it our images and it will return their poses.

We can now generate rays from these camera poses. The idea here is that for input image, we can use its corresponding camera pose to shoot rays into the scene through every pixel we wish to reconstruct. We can then sample points along these rays, pass them into our network (remember that the network takes a point and a direction), and then render a color for each pixel using the network's output along each ray. We can compare our predicted pixel values against the ground truth pixel values taken from our images to calculate a loss and optimize the network. If some of this is confusing now, it will become intuitive when we cover the volume rendering equation and model optimization in further detail.

![NeRF Figure](/images/nerfs/nerf-figure2.png)

### Volume Rendering

This is the volume rendering equation that calculates the color for a given ray:

$$
\begin{equation}
C(\mathbf{r}) = \int_{t_n}^{t_f}T(t)\sigma(\mathbf{r}(t))\mathbf{c}(\mathbf{r}(t),\mathbf{d})dt\,, \textrm{ where }
T(t) = \exp \left({-\int_{t_n}^{t}\sigma(\mathbf{r}(s))ds}\right)
\end{equation}
$$

The equation may look complicated if you have never seen it before but it's actually very interpretable—let’s break it down.

A ray is defined by $\textbf{r}(t) = \textbf{o} + t\textbf{d}$ where $\textbf{o}$ is the origin of the ray, $\textbf{d}$ is the direction of the ray, and $t$ is a scalar. So for any value of $t$, $\textbf{r}(t)$ is a 3D vector representing a position in space.

We can immediately recognize $\sigma(\textbf{r}(t))$ and $\textbf{c}(\textbf{r}(t), \textbf{d})$ in the equation—these are the volume density and color that are predicted by our NeRF in a given forward pass for position $\textbf{r}(t)$ and viewing direction $\textbf{d}$. Intuitively, when the volume density is higher (i.e. there is some object at this point in space), the color of that point will contribute more to the final calculated color of the ray, since those two quantities are multiplied. And similarly, when the volume density is close to zero (i.e. there is nothing at that point in space), that point’s contribution to the final color will be negligible.

The function $T(t)$ is the accumulated transmittance along the ray, which is basically a measure of how much "stuff" has been hit while tracing the ray from $t_n$ to $t$. To visualize its contribution to the final rendered color of the ray, let's set $u = \int_{t_n}^{t}\sigma(\mathbf{r}(s))ds$. This allows us to write the accumulated transmittance as $T(u) = e^{-u}$, which is just the inverse exponential function.

![e^-x](/images/nerfs/e^-x.png)

We can see that $u$ is the sum of the volume density of each infinitesimal particle ($ds$) along the ray. Since volume density cannot be negative (physically and since $\sigma$ is predicted using ReLU), $u$ will always be nonnegative. So the accumulated transmittance value will decrease from 1 as volume density accumulates along the ray. In the context of the volume rendering equation, this makes a lot of sense: once the ray has hit objects, the color contribution of points further along the ray should given less weight, since it is usually hard to see through objects.

We now have mathematical intuition for every part of the volume rendering equation, except for the bounds of the integral. The bounds are straightforward, but I'll still cover them to illustrate some computational points.

![NeRF Scene](/images/nerfs/nerf-scene.001.png)

We calculate the integral from a near bound $t_n$ to a far bound $t_f$. To illustrate the importance of the bounds, imagine that we are training a NeRF on scene images taken from a drone high above an object of interest, as in the picture above. If $t_n$ is too small, then we will be running forward passes on points that are pretty much just air, which has near-zero volume density--since volume density is multiplied by color to calculate each point's color contribution to the final rendered color, these points will effectively have no contribution; this will result in a bunch of wasted computation. Similarly, if $t_f$ is too large, we could be running forward passes on points for which $T(t)$ is very small--more wasted computation.

In implementation, the volume rendering integral is approximated numerically. We split $[t_n, t_f]$ into evenly spaced bins and sample a point uniformly from each bin. The discrete formula is:

$$
\begin{equation}
\hat{C}(\mathbf{r})=\sum_{i=1}^N T_i (1-\exp\left({-\sigma_i \delta_i})\right) \mathbf{c}_i\,, \textrm{ where }
T_i=\exp\left({- \sum_{j=1}^{i-1} \sigma_j \delta_j}\right)\,,
\end{equation}
$$

where $\delta$ represents the distance between adjacent samples. The intuition here is the same as before: points with higher volume density contribute more to the rendered color, and contributions decrease as density accumulates along the ray. The difference is just that the integrals are replaced with sums and $\sigma(\mathbf{r}(t))$ has been replaced with $1 - \exp \left(-\sigma_i \delta_i)\right)$.

### Hierarchical Sampling

In practice, the authors of the original paper jointly optimize two NeRFs: one "coarse" model and one "fine" model. The reason is that, even if we can pick reasonable $t_n$ and $t_f$ bounds for the scene, we will still be running forward passes on points that have near-zero volume density and points that are occluded; this can be mitigated by training a coarse model whose predictions can inform the fine model how to sample points more effectively.

This joint optimization is accomplished by first taking $N_c$ samples along the ray, running them through the coarse network, and then writing the output as a weighted sum:

$$
\begin{equation}
  \hat{C}_c(\mathbf{r})=\sum_{i=1}^{N_c}w_i c_i\,, \quad\,\,
  w_i = T_i(1-\exp\left({-\sigma_i \delta_i})\right)\,.
\end{equation}
$$

Higher $w$ values correspond to a greater contribution to the rendered color. We can normalize these values as $w_i = \frac{w_i}{\sum_{j}^{N_c}w_j}$ to generate a probability distribution where relevant points are weighted higher. Then we can sample $N_f$ points from this distribution (the exact sampling method used is inverse transform sampling). The fine model then runs on these $N_f$ points in addition to the original $N_c$ points.

### Optimization

The loss score used for NeRFs is the mean squared error between predicted RGB pixel values and ground truth pixel values. Note that we don't optimize directly over the output of a single forward pass: for example, in an image classification task a network predicts a probability distribution of classes for a single image and that is used to directly compute a loss; in the case of NeRFs, we make many predictions, perform volume rendering over them, and then compute the loss based on the output of the volume rendering step. The original NeRF uses the Adam optimizer and has a learning rate decay from $5 \times 10^{-4}$ to $5 \times 10^{-5}$.

### Subsequent Work

While the vanilla NeRF described in this post was revolutionary, it has its limitations. It tends to do worse when trained on images of varying scales; it suffers from aliasing effects; it has trouble with images taken in different lighting conditions, etc. Improved NeRF models such as BungeeNeRF, MipNeRF, and NeRFW have been created to tackle these issues. In general, there have been many new developments along this line of work. To keep exploring, I suggest the awesome-nerf repo and Jon Barron's research website.

https://jonbarron.info/

### References

https://arxiv.org/pdf/2003.08934.pdf

https://github.com/yenchenlin/nerf-pytorch
